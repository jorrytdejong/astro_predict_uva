{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b5517e",
   "metadata": {},
   "source": [
    "# Astrology Verification via Language Model\n",
    "\n",
    "This Jupyter notebook introduces an innovative project aimed at testing the validity of astrology. The project involves the use of a language model to analyze the biographies of people born on specific dates, corresponding to each astrological sign, and summarizing their characteristics. These summaries are then used to see if a certain astrological sign could be assigned based on the identified characteristics.\n",
    "\n",
    "The individuals are selected based on their birth date and their fame, ensuring a rich biography for the language model to process. The summaries generated by the language model serve as a character analysis based on the individuals' biographies. The language model is then tasked to assign an astrological sign to each person based on these summaries.\n",
    "\n",
    "The underlying assumptions are that the character analysis based on the biography is accurate and that the correct astrological sign can be determined from these characteristics. This approach offers an intriguing way to examine the claims of astrology through the lens of data analysis and natural language processing.\n",
    "\n",
    "The biographies will be processed through a local Large Language Model, or Ollama. This tool will assign astrology signs, assuming the model has absorbed enough modern astrological information to make similar conclusions about people as typically done in astrology.\n",
    "\n",
    "First, we will create a data file by randomly selecting a certain number of renowned individuals born at the midpoint of an astrological sign. This ensures that the characteristics of the specific sign are at their strongest. We will retrieve the names of these individuals, along with the Wikipedia links to their biographies.\n",
    "\n",
    "Next, we will automatically scrape all the biographies into a CSV or SQL file. This will result in a table containing birthdates, names, and biographies.\n",
    "\n",
    "Afterward, the OLAMA model will extract characteristics from this data. We will ensure the few-shot prompt functions correctly and verify that it provides the required results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cb813",
   "metadata": {},
   "source": [
    "In this section, we'll loop through the biographies and use them as context for deriving personal information. If there's a 'Personal Life' section available in the Wikipedia page, we'll just take this section. Otherwise, we'll use the whole biography. The derived personal information will be inputted into the OLAMA() function, resulting in a list of short characteristics for each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE\n",
    "\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"biographies.csv\")\n",
    "\n",
    "# Initialize Wikipedia API\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "# Loop through biographies\n",
    "characteristics = []\n",
    "for biography in data['Biography']:\n",
    "    # Check for Personal Life section\n",
    "    page_py = wiki_wiki.page(biography)\n",
    "    if 'Personal life' in page_py.sections:\n",
    "        context = page_py.section_by_title('Personal life').text\n",
    "    else:\n",
    "        context = page_py.text\n",
    "\n",
    "    # Use the context for deriving personal information\n",
    "    characteristics.append(OLAMA(context))\n",
    "\n",
    "# Add the characteristics to the data\n",
    "data['Characteristics'] = characteristics\n",
    "\n",
    "# Save the data\n",
    "data.to_csv(\"biographies_with_characteristics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bd089",
   "metadata": {},
   "source": [
    "We will use a method \"few-shot-prompting\" to generate the data that we want for our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c82cfc",
   "metadata": {},
   "source": [
    "So, lets use it now to see if it can generate a astrological analysis, based on the knowledge of astrology that the language model might have. To test it, let's first read an astrological analysis of two to three people, their short biography and then the astrological sign in which they were born. Then we put their biographies as input, we let the model behave like an astrologer, and then see if we end up having the same conclusion. We ask the model to give a reason for why it chose what it chose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fa01a",
   "metadata": {},
   "source": [
    "We have to erase the names of the persons. Write a method for that. It is possible to cut out the name of the person manually, but then the text is still so descriptive, that the language model will know who it is a bout, including the date of birth.\n",
    "\n",
    "So the characteristics first go through a prompt like this: \"Describe these traits as if they were from a random person\n",
    "... , make no reference to Barack Obama\"\n",
    "\n",
    "\n",
    "But then still, much reference is made to the activities of the person and it could be easily known what he/she has done, that's why we throw it through another prompt. \"Just summarize the characteristics of the person, without mentioning in any \n",
    "... way examples of his/her behavior\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59e3a8",
   "metadata": {},
   "source": [
    "# Using Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d68db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r8_dnYrQg9fYdsHhyJpejnzmJBUcWWCbWT2zVyLT\n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the REPLICATE_API_TOKEN environment variable\n",
    "os.environ['REPLICATE_API_TOKEN'] = 'r8_dnYrQg9fYdsHhyJpejnzmJBUcWWCbWT2zVyLT'\n",
    "\n",
    "# Verify that the environment variable is set\n",
    "print(os.environ['REPLICATE_API_TOKEN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "389158e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pisces'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replicate_llama70b(prompt):\n",
    "    input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    }\n",
    "\n",
    "    output = replicate.run(\n",
    "        \"meta/meta-llama-3-70b-instruct\",\n",
    "        input=input\n",
    "    )\n",
    "    return \"\".join(output)\n",
    "\n",
    "def characteristics_of(person):\n",
    "    prompt = f\"\"\" Describe the positive and negative character traits of {person}\n",
    "            \"\"\"\n",
    "    \n",
    "    answer = replicate_llama70b(prompt)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def unpersonal_characteristics(characteristics):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    positive and negative traits: \"{characteristics}\"\n",
    "\n",
    "    Based on these positive and negative traits, make\n",
    "    a general overview of characteristics while making no reference to {person}. Just summarize the characteristics \n",
    "    of the person. So don't mention in any way examples of his/her \n",
    "    behavior. \"\"\"\n",
    "    \n",
    "    answer = replicate_llama70b(prompt)\n",
    "\n",
    "    return answer\n",
    "\n",
    "def assign_zodiac_to(traits):\n",
    "    prompt = f\"\"\" traits: \"{traits}\"\"\n",
    "            question: \"What could be the astrology sign of this person based on these traits?\"\n",
    "            answer: [just answer with one word, for example: \"Pisces\", \"Virgo\", not two!]\n",
    "\n",
    "            \"\"\"\n",
    "    answer = replicate_llama70b(prompt)\n",
    "    return answer\n",
    "\n",
    "def predicted_astro_sign(person):\n",
    "    \n",
    "    # generate characteristics of the person\n",
    "    characteristics = characteristics_of(person)\n",
    "    \n",
    "    # transform the characteristics to unpersonal traits\n",
    "    unpersonal_traits = unpersonal_characteristics(characteristics)\n",
    "    \n",
    "    # draw a zodiac sign based on the traits\n",
    "    astro_sign = assign_zodiac_to(unpersonal_traits)\n",
    "    \n",
    "    #TODO: write something to redo the last method in case \n",
    "    # it does not output a single word\n",
    "    \n",
    "    return astro_sign\n",
    "\n",
    "\n",
    "person = 'Vincent van Gogh'\n",
    "predicted_astro_sign(person)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d3e84",
   "metadata": {},
   "source": [
    "# Scraping Astro websites\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42d4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Name   Sign\n",
      "0  Matthew Healy  Aries\n",
      "1      Lady Gaga  Aries\n",
      "2  Conan O'Brien  Aries\n",
      "3          Quavo  Aries\n",
      "4   Mariah Carey  Aries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "astrological_signs = [\n",
    "    \"Aries\",\n",
    "    \"Taurus\",\n",
    "    \"Gemini\",\n",
    "    \"Cancer\",\n",
    "    \"Leo\",\n",
    "    \"Virgo\",\n",
    "    \"Libra\",\n",
    "    \"Scorpio\",\n",
    "    \"Sagittarius\",\n",
    "    \"Capricorn\",\n",
    "    \"Aquarius\",\n",
    "    \"Pisces\"\n",
    "]\n",
    "\n",
    "\n",
    "# List to store data\n",
    "data = []\n",
    "\n",
    "for sign in astrological_signs:\n",
    "    source = requests.get(f\"https://astro-charts.com/persons/top/{sign.lower()}/\").text\n",
    "    soup = BeautifulSoup(source, 'html')\n",
    "    for match in soup.find_all('div', class_=\"celeb-info\"):\n",
    "        name = match.find_all('p')[1].text\n",
    "        data.append({\"Name\": name, \"Sign\": sign})\n",
    "\n",
    "        # Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "# df.to_csv('astrological_signs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72c562b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name    Sign Predicted\n",
      "0         Matthew Healy   Aries     Libra\n",
      "1             Lady Gaga   Aries   Scorpio\n",
      "2         Conan O'Brien   Aries    Gemini\n",
      "3                 Quavo   Aries       Leo\n",
      "4          Mariah Carey   Aries       Leo\n",
      "...                 ...     ...       ...\n",
      "1195      Ewan McGregor  Pisces      None\n",
      "1196      Ty Dolla Sign  Pisces      None\n",
      "1197        Gary Oldman  Pisces      None\n",
      "1198       Julia Stiles  Pisces      None\n",
      "1199  Matthew Broderick  Pisces      None\n",
      "\n",
      "[1200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the new column with default values (e.g., None)\n",
    "df['Predicted'] = None\n",
    "\n",
    "# Loop through the DataFrame using basic indexing\n",
    "for i in range(5):\n",
    "    name = df.iloc[i, 0]  # Get the name from the first column\n",
    "    predicted_result = predicted_astro_sign(name)\n",
    "    df.iloc[i, 2] = predicted_result  # Set the predicted result in the third column\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf4eb3",
   "metadata": {},
   "source": [
    "# Glove embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d0a5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data and labels\n",
    "texts = [\"This is a positive text\", \"This is a negative text\"]\n",
    "labels = [1, 0]  # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef999047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings (e.g., 100-dimensional vectors)\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38fb216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06c7c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to padded sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "614f286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 100)            700       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43005 (167.99 KB)\n",
      "Trainable params: 42305 (165.25 KB)\n",
      "Non-trainable params: 700 (2.73 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=padded_sequences.shape[1],\n",
    "              trainable=False),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9f659f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 0.7019 - accuracy: 0.5000 - val_loss: 0.6812 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6812 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 0.6407 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6407 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6201 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences, labels, epochs=5, validation_data=(padded_sequences, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ebdccaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "Text: 'This text is very positive' - Prediction: 0.5329461097717285\n",
      "Text: 'This text is very negative' - Prediction: 0.4332428574562073\n"
     ]
    }
   ],
   "source": [
    "# New texts for prediction\n",
    "new_texts = [\"This text is very positive\", \"This text is very negative\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=padded_sequences.shape[1])\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "\n",
    "for text, prediction in zip(new_texts, predictions):\n",
    "    print(f\"Text: '{text}' - Prediction: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
