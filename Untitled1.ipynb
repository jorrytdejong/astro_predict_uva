{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b5517e",
   "metadata": {},
   "source": [
    "# Astrology Verification via Language Model\n",
    "\n",
    "This Jupyter notebook introduces an innovative project aimed at testing the validity of astrology. The project involves the use of a language model to analyze the biographies of people born on specific dates, corresponding to each astrological sign, and summarizing their characteristics. These summaries are then used to see if a certain astrological sign could be assigned based on the identified characteristics.\n",
    "\n",
    "The individuals are selected based on their birth date and their fame, ensuring a rich biography for the language model to process. The summaries generated by the language model serve as a character analysis based on the individuals' biographies. The language model is then tasked to assign an astrological sign to each person based on these summaries.\n",
    "\n",
    "The underlying assumptions are that the character analysis based on the biography is accurate and that the correct astrological sign can be determined from these characteristics. This approach offers an intriguing way to examine the claims of astrology through the lens of data analysis and natural language processing.\n",
    "\n",
    "The biographies will be processed through a local Large Language Model, or Ollama. This tool will assign astrology signs, assuming the model has absorbed enough modern astrological information to make similar conclusions about people as typically done in astrology.\n",
    "\n",
    "First, we will create a data file by randomly selecting a certain number of renowned individuals born at the midpoint of an astrological sign. This ensures that the characteristics of the specific sign are at their strongest. We will retrieve the names of these individuals, along with the Wikipedia links to their biographies.\n",
    "\n",
    "Next, we will automatically scrape all the biographies into a CSV or SQL file. This will result in a table containing birthdates, names, and biographies.\n",
    "\n",
    "Afterward, the OLAMA model will extract characteristics from this data. We will ensure the few-shot prompt functions correctly and verify that it provides the required results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cb813",
   "metadata": {},
   "source": [
    "In this section, we'll loop through the biographies and use them as context for deriving personal information. If there's a 'Personal Life' section available in the Wikipedia page, we'll just take this section. Otherwise, we'll use the whole biography. The derived personal information will be inputted into the OLAMA() function, resulting in a list of short characteristics for each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE\n",
    "\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import wikipediaapi\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"biographies.csv\")\n",
    "\n",
    "# Initialize Wikipedia API\n",
    "wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "# Loop through biographies\n",
    "characteristics = []\n",
    "for biography in data['Biography']:\n",
    "    # Check for Personal Life section\n",
    "    page_py = wiki_wiki.page(biography)\n",
    "    if 'Personal life' in page_py.sections:\n",
    "        context = page_py.section_by_title('Personal life').text\n",
    "    else:\n",
    "        context = page_py.text\n",
    "\n",
    "    # Use the context for deriving personal information\n",
    "    characteristics.append(OLAMA(context))\n",
    "\n",
    "# Add the characteristics to the data\n",
    "data['Characteristics'] = characteristics\n",
    "\n",
    "# Save the data\n",
    "data.to_csv(\"biographies_with_characteristics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1c959",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8a2b0",
   "metadata": {},
   "source": [
    "Invoking the local model on the macbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke('What is 2 + 2?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bd089",
   "metadata": {},
   "source": [
    "We will use a method \"few-shot-prompting\" to generate the data that we want for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
    "]\n",
    "\n",
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a wondrous wizard of math. \n",
    "                        Just use the Human and AI conversation as \n",
    "                        an example and answer just to the latest question\"\"\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = final_prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"What's the square of a triangle?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c82cfc",
   "metadata": {},
   "source": [
    "So, lets use it now to see if it can generate a astrological analysis, based on the knowledge of astrology that the language model might have. To test it, let's first read an astrological analysis of two to three people, their short biography and then the astrological sign in which they were born. Then we put their biographies as input, we let the model behave like an astrologer, and then see if we end up having the same conclusion. We ask the model to give a reason for why it chose what it chose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764fa01a",
   "metadata": {},
   "source": [
    "We have to erase the names of the persons. Write a method for that. It is possible to cut out the name of the person manually, but then the text is still so descriptive, that the language model will know who it is a bout, including the date of birth.\n",
    "\n",
    "So the characteristics first go through a prompt like this: \"Describe these traits as if they were from a random person\n",
    "... , make no reference to Barack Obama\"\n",
    "\n",
    "\n",
    "But then still, much reference is made to the activities of the person and it could be easily known what he/she has done, that's why we throw it through another prompt. \"Just summarize the characteristics of the person, without mentioning in any \n",
    "... way examples of his/her behavior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602289c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59e3a8",
   "metadata": {},
   "source": [
    "# Using Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d68db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r8_dnYrQg9fYdsHhyJpejnzmJBUcWWCbWT2zVyLT\n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the REPLICATE_API_TOKEN environment variable\n",
    "os.environ['REPLICATE_API_TOKEN'] = 'r8_dnYrQg9fYdsHhyJpejnzmJBUcWWCbWT2zVyLT'\n",
    "\n",
    "# Verify that the environment variable is set\n",
    "print(os.environ['REPLICATE_API_TOKEN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "757cf6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Describe the positive and negative character traits of Billie Eilish\n",
      "\n",
      "Billie Eilish is a talented and influential artist known for her unique style, music, and persona. Here are some positive and negative character traits associated with Billie Eilish:\n",
      "\n",
      "Positive Traits:\n",
      "\n",
      "1. Authenticity: Billie Eilish is unapologetically herself, refusing to conform to industry standards or societal expectations. She stays true to her art and vision, which resonates with her fans.\n",
      "2. Creativity: Eilish is a creative genius, constantly pushing boundaries with her music, visuals, and performances. Her innovative approach has inspired a new generation of artists and fans.\n",
      "3. Confidence: Billie exudes confidence and self-assurance, which is infectious and empowering to her audience. She's not afraid to speak her mind and stand up for what she believes in.\n",
      "4. Empathy: Eilish has shown compassion and understanding towards her fans, often using her platform to raise awareness about mental health, environmental issues, and social justice.\n",
      "5. Humor: Billie has a great sense of humor, often incorporating wit and sarcasm into her interviews, social media, and music videos.\n",
      "6. Loyalty: Eilish is fiercely loyal to her family, friends, and team, often crediting them for their support and collaboration.\n",
      "7. Passion: Billie is passionate about her craft, pouring her heart and soul into every project she takes on. Her enthusiasm is contagious and inspires her fans to pursue their own passions.\n",
      "\n",
      "Negative Traits:\n",
      "\n",
      "1. Intensity: Billie Eilish's intense personality can sometimes be overwhelming or off-putting to those who don't know her. She's been known to be brutally honest and direct, which can be perceived as abrasive.\n",
      "2. Moodiness: Eilish has been open about her struggles with depression and anxiety, which can affect her mood and behavior. She's been known to cancel shows or take breaks from social media due to her mental health.\n",
      "3. Defensiveness: Billie can be defensive when criticized or questioned about her art or opinions. She's been involved in public feuds and controversies, which can be seen as immature or unprofessional.\n",
      "4. Rebelliousness: Eilish's desire to challenge the status quo and push boundaries can sometimes lead to reckless or impulsive decisions, which may not always be well-received by her fans or the media.\n",
      "5. Perfectionism: Billie has high standards for herself and others, which can lead to frustration and burnout. She's been open about her\n"
     ]
    }
   ],
   "source": [
    "person = \"Billie Eilish\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\" Describe the positive and negative character traits of {person}\n",
    "\"\"\"\n",
    "print(prompt)\n",
    "\n",
    "\n",
    "input = {\n",
    "    \"prompt\": prompt,\n",
    "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "}\n",
    "\n",
    "output = replicate.run(\n",
    "    \"meta/meta-llama-3-70b-instruct\",\n",
    "    input=input\n",
    ")\n",
    "first_output = \"\".join(output)\n",
    "print(first_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "389158e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pisces'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replicate_llama70b(prompt):\n",
    "    input = {\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    }\n",
    "\n",
    "    output = replicate.run(\n",
    "        \"meta/meta-llama-3-70b-instruct\",\n",
    "        input=input\n",
    "    )\n",
    "    return \"\".join(output)\n",
    "\n",
    "def characteristics_of(person):\n",
    "    prompt = f\"\"\" Describe the positive and negative character traits of {person}\n",
    "            \"\"\"\n",
    "    \n",
    "    answer = replicate_llama70b(prompt)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def unpersonal_characteristics(characteristics):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    positive and negative traits: \"{characteristics}\"\n",
    "\n",
    "    Based on these positive and negative traits, make\n",
    "    a general overview of characteristics while making no reference to {person}. Just summarize the characteristics \n",
    "    of the person. So don't mention in any way examples of his/her \n",
    "    behavior. \"\"\"\n",
    "    \n",
    "    answer = replicate_llama70b(prompt)\n",
    "\n",
    "    return answer\n",
    "\n",
    "def assign_zodiac_to(traits):\n",
    "    prompt = f\"\"\" traits: \"{traits}\"\"\n",
    "            question: \"What could be the astrology sign of this person based on these traits?\"\n",
    "            answer: [just answer with one word, for example: \"Pisces\", \"Virgo\", not two!]\n",
    "\n",
    "            \"\"\"\n",
    "    answer = replicate_llama70b(prompt)\n",
    "    return answer\n",
    "\n",
    "def predicted_astro_sign(person):\n",
    "    \n",
    "    # generate characteristics of the person\n",
    "    characteristics = characteristics_of(person)\n",
    "    \n",
    "    # transform the characteristics to unpersonal traits\n",
    "    unpersonal_traits = unpersonal_characteristics(characteristics)\n",
    "    \n",
    "    # draw a zodiac sign based on the traits\n",
    "    astro_sign = assign_zodiac_to(unpersonal_traits)\n",
    "    \n",
    "    #TODO: write something to redo the last method in case \n",
    "    # it does not output a single word\n",
    "    \n",
    "    return astro_sign\n",
    "\n",
    "\n",
    "person = 'Vincent van Gogh'\n",
    "predicted_astro_sign(person)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d3e84",
   "metadata": {},
   "source": [
    "# Scraping Astro websites\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42d4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Name   Sign\n",
      "0  Matthew Healy  Aries\n",
      "1      Lady Gaga  Aries\n",
      "2  Conan O'Brien  Aries\n",
      "3          Quavo  Aries\n",
      "4   Mariah Carey  Aries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "astrological_signs = [\n",
    "    \"Aries\",\n",
    "    \"Taurus\",\n",
    "    \"Gemini\",\n",
    "    \"Cancer\",\n",
    "    \"Leo\",\n",
    "    \"Virgo\",\n",
    "    \"Libra\",\n",
    "    \"Scorpio\",\n",
    "    \"Sagittarius\",\n",
    "    \"Capricorn\",\n",
    "    \"Aquarius\",\n",
    "    \"Pisces\"\n",
    "]\n",
    "\n",
    "\n",
    "# List to store data\n",
    "data = []\n",
    "\n",
    "for sign in astrological_signs:\n",
    "    source = requests.get(f\"https://astro-charts.com/persons/top/{sign.lower()}/\").text\n",
    "    soup = BeautifulSoup(source, 'html')\n",
    "    for match in soup.find_all('div', class_=\"celeb-info\"):\n",
    "        name = match.find_all('p')[1].text\n",
    "        data.append({\"Name\": name, \"Sign\": sign})\n",
    "\n",
    "        # Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "# df.to_csv('astrological_signs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72c562b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name    Sign Predicted\n",
      "0         Matthew Healy   Aries     Libra\n",
      "1             Lady Gaga   Aries   Scorpio\n",
      "2         Conan O'Brien   Aries    Gemini\n",
      "3                 Quavo   Aries       Leo\n",
      "4          Mariah Carey   Aries       Leo\n",
      "...                 ...     ...       ...\n",
      "1195      Ewan McGregor  Pisces      None\n",
      "1196      Ty Dolla Sign  Pisces      None\n",
      "1197        Gary Oldman  Pisces      None\n",
      "1198       Julia Stiles  Pisces      None\n",
      "1199  Matthew Broderick  Pisces      None\n",
      "\n",
      "[1200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the new column with default values (e.g., None)\n",
    "df['Predicted'] = None\n",
    "\n",
    "# Loop through the DataFrame using basic indexing\n",
    "for i in range(5):\n",
    "    name = df.iloc[i, 0]  # Get the name from the first column\n",
    "    predicted_result = predicted_astro_sign(name)\n",
    "    df.iloc[i, 2] = predicted_result  # Set the predicted result in the third column\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf4eb3",
   "metadata": {},
   "source": [
    "# Glove embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d0a5b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data and labels\n",
    "texts = [\"This is a positive text\", \"This is a negative text\"]\n",
    "labels = [1, 0]  # 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef999047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings (e.g., 100-dimensional vectors)\n",
    "embedding_index = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38fb216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06c7c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to padded sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "614f286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 100)            700       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                42240     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43005 (167.99 KB)\n",
      "Trainable params: 42305 (165.25 KB)\n",
      "Non-trainable params: 700 (2.73 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(word_index) + 1,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=padded_sequences.shape[1],\n",
    "              trainable=False),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9f659f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 1s 820ms/step - loss: 0.7019 - accuracy: 0.5000 - val_loss: 0.6812 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6812 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 0.6407 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6407 - accuracy: 1.0000 - val_loss: 0.6201 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6201 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_sequences, labels, epochs=5, validation_data=(padded_sequences, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ebdccaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 135ms/step\n",
      "Text: 'This text is very positive' - Prediction: 0.5329461097717285\n",
      "Text: 'This text is very negative' - Prediction: 0.4332428574562073\n"
     ]
    }
   ],
   "source": [
    "# New texts for prediction\n",
    "new_texts = [\"This text is very positive\", \"This text is very negative\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=padded_sequences.shape[1])\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "\n",
    "for text, prediction in zip(new_texts, predictions):\n",
    "    print(f\"Text: '{text}' - Prediction: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
